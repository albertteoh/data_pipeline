---

# Base config
# ###########

# Source database type: [ oracle | mssql | postgres | greenplum ]
sourcedbtype: oracle

# Source system identifier (code)
sourcesystem: MYSOURCESYSTEM


# Schema name containing process audit tables
auditschema: auditschema

# Process audit user credentials requried for logging processing metrics
audituser: postgres/password@db-postgres-audit:5432/myaudit


# Stream host name / kafka cluster host
streamhost: broker:9092

# Stream channel name / kafka topic
streamchannel: mytopic

# Stream group identifer / kafka consumer group
streamgroup: mygroup

# Stream schema host name / kafka cluster host
streamschemahost: http://schema_registry:8081

# Stream schema file
streamschemafile: /usr/local/data_pipeline/data_pipeline/stream/OracleCDCSchema.avsc


# List of table(s) held as an application profile in process
# control database
profilename: myprofilename

# Application profile version number
profileversion: 1


# Output working directory
workdirectory: /tmp

# Verbose mode
verbose: True


# Hostname of the smtp server for sending emails
# notifysmtpserver: mysmtphost.corp

# Sender email address used when sending notification emails
# notifysender: datapipeline_prod@mail.com

# List of recipient email addresses who will
# receive notifications upon an error in the application
# notifyerrorlist: [alice@error.com, bob@error.com]

# List of recipient email addresses who will
# receive notifications of summary details and statistics upon
# completion of a run
# notifysummarylist: [alice@summary.com, bob@summary.com]


# When looping over a large number of records,
# this will log audit updates every given number of records
auditcommitpoint: 50

# Name of file where data is written to prior to being sent to
# an external source
outputfile: applier_output.dat

# Name of file where raw extract source output is written to
rawfile: applier_output.raw

# Use this sslmode for the database connection
# (default: prefer)
# sslmode: prefer

# The file path of the client SSL certificate
# (default: $HOME/.postgresql/postgresql.crt)
# sslcert: /path/to/postgresql.crt

# The  file path of the secret key used for the client certificate
# (default: $HOME/.postgresql/postgresql.key)
# sslkey: /path/to/postgresql.key

# This parameter specifies the file name of the SSL certificate
# revocation list (CRL). Certificates listed in this file, if it
# exists, will be rejected while attempting to authenticate the
# server's certificate
# (default: $HOME/.postgresql/root.crl)
# sslcrl: /path/to/root.crl

# source data character encoding
clientencoding: utf-8

# Applier specific config
# #######################

# Target database user credentials in the form: dbuser/dbpasswd@SRCSID[:PORT]
targetuser: postgres/password@db-postgres-dest:5432/mydestination

# Schema name where target tables reside (default: None)
targetschema: destschema

# Target database type: [ oracle | mssql | greenplum | postgres ]
targetdbtype: postgres


# Full path to the yaml config file containing the source->target data type
# mappings
datatypemap: /usr/local/data_pipeline/conf/postgres_datatype_mappings.yaml

# When looping over a large number of records, this will
# cause a commit of executed transactions on target
# every given number of records
targetcommitpoint: 50

# When the applier exits due to an ERROR (environmental,
# data etc), prior to exiting - retry execution from last
# statement (the statement where error was detected) this
# many times prior to exiting
retry: 0

# Enables bulk applying of DML statements for improved
# performance
bulkapply: True


# Enable metadata columns to be populated on target. It must be in
# json format of supported metadata columns paired with their
# respective target metadata column names.
# For example: { "insert_timestamp_column": "ctl_ins_ts", "update_timestamp_column": "ctl_upd_ts" }.
# Supported metadata columns are:
#   [insert_timestamp_column, update_timestamp_column]
# (default: None)
# metacols: { "insert_timestamp_column" : "ctl_ins_ts", "update_timestamp_column" : "ctl_upd_ts" }

# Seek to the end of the kafka queue
seektoend: False

# Skips the given number of batches without processing,
# while still committing the offset
skipbatch: 0
